#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "Arial"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts true
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing double
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\align center

\size huge
METAM Algorithm Implementation and Analysis
\end_layout

\begin_layout Standard
\align center

\size large
as proposed in the paper ‚ÄúGoal-Oriented Data Discovery‚Äù [Sainyam Galhotra
 et al.]
\end_layout

\begin_layout Standard
\align center
Sofiya Shtetenson and Timna Smadja 
\end_layout

\begin_layout Standard
\align center

\series bold
\size large
\color blue
\begin_inset CommandInset href
LatexCommand href
name "GitHub"
target "https://github.com/timnas/METAM_algorithm.git"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\align center

\series bold
\size large
\color blue
\begin_inset CommandInset href
LatexCommand href
name "Streamlit App"
target "https://metam-algorithm-analysis.streamlit.app/"
literal "false"

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
In modern data-driven decision-making, the availability of large and diverse
 datasets presents both an opportunity and a challenge.
 While more data can potentially improve predictive performance, integrating
 heterogeneous data sources in a meaningful way is nontrivial.
 In this work, we implement and evaluate METAM (Minimal Essential Task Augmentat
ion Mechanism)‚Äîa goal-oriented data discovery algorithm that iteratively
 identifies and joins external candidate datasets to improve a downstream
 task‚Äôs performance (for example - classification accuracy).
 By selectively augmenting the core dataset with relevant features, METAM
 automates what data scientists often do manually‚Äîsearching for beneficial
 external data, joining it, and then observing performance gains.
\end_layout

\begin_layout Section
Background
\end_layout

\begin_layout Standard
Traditional data augmentation methods often rely on manual feature engineering
 or pre-defined heuristics to select supplementary datasets.
 However, these approaches can be inefficient and may fail to capture the
 true incremental value of additional data.
 The METAM algorithm, as described in ‚ÄúMETAM: Goal-Oriented Data Discovery‚Äù
 by Sainyam Galhotra et al., introduces a feedback loop in which candidate
 augmentations are evaluated based on their effect on a downstream utility
 function.
 Key aspects of the approach include:
\end_layout

\begin_layout Itemize
Data Profiles: Each candidate augmentation is characterized by task-independent
 measures such as join key overlap, data completeness, and feature richness.
 
\end_layout

\begin_layout Itemize
Adaptive Querying: The algorithm alternates between evaluating individual
 candidates and groups (via clustering) to efficiently explore the augmentation
 space.
 
\end_layout

\begin_layout Itemize
Minimal Augmentation Set: Augmentations are only incorporated if they yield
 a significant improvement in the target task‚Äôs utility, ensuring that the
 final augmented dataset is both parsimonious and effective.
 
\end_layout

\begin_layout Section
Methodology Overview 
\end_layout

\begin_layout Subsection
Problem Formulation
\end_layout

\begin_layout Standard
Given a base dataset 
\begin_inset Formula $D_{base}$
\end_inset

 and a repository of candidate datasets 
\begin_inset Formula $\{D_{1},D_{2},...,D_{n}\}$
\end_inset

, the goal is to select a minimal subset 
\begin_inset Formula $T\subseteq\{D_{i}\}$
\end_inset

 such that the augmented dataset 
\begin_inset Formula $\Gamma(D_{base},T)$
\end_inset

 achieves a utility 
\begin_inset Formula $ùë¢(Œì(ùê∑base,ùëá))‚â•ùúÉ$
\end_inset

, where 
\begin_inset Formula $\theta$
\end_inset

 is a predetermined threshold (e.g., based on accuracy for classification
 tasks).
\end_layout

\begin_layout Subsection
Algorithm Overview
\end_layout

\begin_layout Standard
The METAM algorithm proceeds as follows:
\end_layout

\begin_layout Enumerate
Candidate Generation: 
\end_layout

\begin_deeper
\begin_layout Standard
Each candidate augmentation is processed to compute a vector of data profiles
 that includes: 
\end_layout

\begin_layout Enumerate
Join Key Overlap: The ratio of common join keys between the base dataset
 and the candidate.
 
\end_layout

\begin_layout Enumerate
Completeness: 1 minus the missing rate in the candidate‚Äôs join key.
 
\end_layout

\begin_layout Enumerate
Normalized Column Count: The number of columns in the candidate dataset
 divided by a normalization constant.
 
\end_layout

\begin_layout Enumerate
Semantic Similarity: A measure (derived from cosine similarity of SentenceTransf
ormer embeddings) of how semantically similar the candidate‚Äôs column names
 are to those in the base dataset.
 
\end_layout

\begin_layout Standard
An initial quality score is then assigned as the mean of these profile values.
\end_layout

\end_deeper
\begin_layout Enumerate
Clustering: 
\end_layout

\begin_deeper
\begin_layout Standard
Candidates are clustered based on the similarity of their profile vectors.
 This step reduces redundant evaluations by grouping together augmentations
 that are expected to yield similar effects on the utility function.
\end_layout

\end_deeper
\begin_layout Enumerate
Adaptive Querying: 
\end_layout

\begin_deeper
\begin_layout Standard
The algorithm iteratively queries the utility function by merging candidate
 augmentations with the base dataset.
 During each iteration, candidates are evaluated both individually and as
 part of groups.
 Only those augmentations that yield a measurable improvement in utility
 (exceeding a specified threshold ùúÉ) are selected.
\end_layout

\end_deeper
\begin_layout Enumerate
Minimality Check:
\end_layout

\begin_deeper
\begin_layout Standard
After candidate selection, a minimality check is performed by attempting
 to remove each selected augmentation.
 If removing an augmentation does not reduce the overall utility below the
 threshold ùúÉ, it is deemed redundant and is removed from the final augmentation
 set.
\end_layout

\end_deeper
\begin_layout Subsection
Implementation Details 
\end_layout

\begin_layout Standard
Our implementation of METAM is in Python and integrates a Streamlit UI for
 interactive experimentation.
 
\end_layout

\begin_layout Standard
Key implementation details include:
\end_layout

\begin_layout Itemize
Utility Function: 
\end_layout

\begin_deeper
\begin_layout Standard
The utility function is computed using an 80/20 train-test split.
 For classification tasks, logistic regression accuracy is used as the performan
ce metric.
\end_layout

\begin_layout Standard
For regression tasks, Linear Regression‚Äôs R¬≤ score is employed.
 Non-numeric features and missing values are handled via filtering and imputatio
n.
\end_layout

\end_deeper
\begin_layout Itemize
Streamlit UI: 
\end_layout

\begin_deeper
\begin_layout Standard
The UI is written in python and built using Streamlit.
 It allows the user to select from the experiments detailed in 
\bar under
section 4
\bar default
 and run the algorithms on them.
 The UI displays candidate augmentation details‚Äîincluding their computed
 profiles and quality scores‚Äîin a table, and presents the final results.
 In Addition, the user can adjust the threshold 
\begin_inset Formula $\theta$
\end_inset

 and see how it affects the results of the experiments.
\end_layout

\end_deeper
\begin_layout Section
Experimental Setup
\end_layout

\begin_layout Enumerate

\bar under
Feature Engineering
\bar default
: Classification - Expensive Housing in Boston - Using Partitioned Features
\end_layout

\begin_deeper
\begin_layout Standard
In this experiment, we adopt a novel approach by taking a single, feature‚Äërich
 dataset (the Boston Housing dataset) and partitioning it into distinct
 feature subjects.
 Specifically, we split the dataset into three groups, then use a reduced
 version of the dataset as our base.
 The candidate augmentations, which are the partitioned feature groups,
 are then merged back into the base data using METAM.
\end_layout

\begin_layout Standard
This experimental setting is significant for the task of feature engineering,
 in the 
\bar under
feature reduction perspective
\bar default
.
\end_layout

\begin_layout Standard
This ‚Äúreduction‚Äù capability highlights METAM‚Äôs ability to achieve a minimal
 yet effective augmentation set, a concept discussed in the original paper
 in the context of minimality of the augmentation set.
 While the paper primarily focuses on goal-oriented data discovery, 
\series bold
our experiment demonstrates that the same framework can be leveraged to
 reduce redundancy and perform implicit feature selection
\end_layout

\begin_layout Itemize

\bar under
Base Set:
\bar default
 'RM' (average number of rooms per dwelling) and 'LSTAT' (percentage of
 lower status of the population)‚Äîalong with a dummy join key "Id" and a
 binary target "expensive" (1 if MEDV is at or above the median, 0 otherwise).
\end_layout

\begin_layout Itemize

\bar under
Task:
\bar default
 predicting the likelihood of housing to be expensive (greater than the
 median)
\end_layout

\begin_layout Itemize

\bar under
Augementation candidates:
\bar default
 The remaining features were grouped into three candidate subjects:
\end_layout

\begin_deeper
\begin_layout Itemize
Crime/Industrial Features (CRIM, INDUS, NOX, AGE, DIS.)
\end_layout

\begin_layout Itemize
Zoning/Tax Features) (ZN, RAD, TAX, PTRATIO.)
\end_layout

\begin_layout Itemize
Structural/Demographic Features (CHAS,B)
\end_layout

\end_deeper
\begin_layout Itemize

\bar under
Results:
\bar default
 Selected augmentation: Zoning/Tax Features
\end_layout

\begin_layout Itemize

\bar under
Explanation:
\bar default
 
\end_layout

\begin_deeper
\begin_layout Standard
Initial Base Utility: The base model using only 'RM' and 'LSTAT' achieved
 an accuracy of 81.37%.
 Iteration 1: Merging Candidate 1 resulted in a utility of 84.31% (gain of
 2.94%).
 In the grouping step, Candidate 2 from the identified group yielded a higher
 utility of 85.29%.
 The algorithm selected Candidate 2 (Zoning/Tax Features), which improved
 the base utility from 81.37% to 85.29%.
 A minimality check confirmed that removing this augmentation dropped the
 utility back to 81.37%, verifying its contribution.
 
\end_layout

\begin_layout Standard
Final Performance: 85.29% accuracy, with "Zoning/Tax Features" selected as
 the effective augmentation.
 
\end_layout

\end_deeper
\begin_layout Itemize

\bar under
Conclusion:
\bar default
 This experiment demonstrates that when a rich dataset is partitioned into
 distinct feature groups, METAM can be used for feature reduction‚Äîeffectively
 identifying which groups add incremental predictive power and which are
 redundant.
 In this case, even though all candidate profiles showed high overlap and
 data completeness, the algorithm selected the "Zoning/Tax Features" augmentatio
n because it provided a measurable improvement in accuracy.
 This result aligns with the goal-oriented nature of METAM as described
 in the paper, emphasizing the minimality of the augmentation set: augmentations
 are incorporated only when they significantly enhance the utility of the
 model.
\end_layout

\end_deeper
\begin_layout Enumerate

\bar under
The augmentation set is minimal
\bar default
: Expensive Housing Classification in Seattle 
\end_layout

\begin_deeper
\begin_layout Itemize

\bar under
Base set
\bar default
: seattle_housing_prices
\end_layout

\begin_layout Itemize

\bar under
Augementation candidates:
\bar default
 seattle_incomes, seattle_pet_licenses, seattle_crime_rates,
\end_layout

\begin_layout Itemize

\bar under
Task:
\bar default
 predicting the likelihood of housing to be expensive (greater than the
 medain) based on zip codes
\end_layout

\begin_layout Itemize

\bar under
Result:
\bar default
 no augmentation was selected
\end_layout

\begin_layout Itemize

\bar under
Explanation:
\bar default
 The base model already achieved a very high predictive utility (98.02% accuracy).
 
\end_layout

\begin_deeper
\begin_layout Standard
We computed candidate profiles that measured the overlap of join keys, data
 completeness, and relative feature richness.
 For instance, the crime rate candidate had a high overlap (0.93) and a quality
 score of 0.65.
 Despite these favorable attributes, when we merged the candidate into the
 base dataset, the observed gain in utility was negligible.
\end_layout

\begin_layout Standard
As we can see, because the base dataset already yielded near-optimal performance
, the additional features did not increase the utility sufficiently.
 Therefore, the algorithm correctly concluded that further augmentation
 was unnecessary and did not select any candidate.
\end_layout

\begin_layout Standard
This behavior is consistent with the discussion in the paper on the 
\bar under
monotonicity of the utility function
\bar default
 and the 
\bar under
minimality of the augmentation set
\bar default
: when the base data already approaches optimal predictive performance,
 additional augmentations may be redundant.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate

\bar under
Goal-oriented nature of METAM
\bar default
 - High Cat Ratio Classification
\end_layout

\begin_deeper
\begin_layout Itemize

\bar under
Base set:
\bar default
 seattle_pet_licenses (containing only ‚Äúzipcode‚Äù and a weak predictor, ‚Äútotal_pe
ts‚Äù)
\end_layout

\begin_layout Itemize

\bar under
Augementation candidates:
\bar default
 seattle_incomes, seattle_housing_prices, seattle_crime_rates
\end_layout

\begin_layout Itemize

\bar under
Task:
\bar default
 predicting the likelihood of having a high proportion of cats per zipcode
\end_layout

\begin_layout Itemize

\bar under
Result:
\bar default
 selected augmentation: seattle_incomes
\end_layout

\begin_layout Itemize

\bar under
Explanation:
\bar default
 
\end_layout

\begin_deeper
\begin_layout Standard
In this experiment we used exactly the same dataset as in the previous example,
 but for a different task.
 
\end_layout

\begin_layout Standard
The base dataset produced a modest utility of 0.6216.
 By incorporating the seattle_incomes dataset as a candidate augmentation
 METAM detected a measurable gain, improving the utility to 0.6410 and thus
 selected the augmentation.
\end_layout

\end_deeper
\begin_layout Standard
These two examples contrast highlights the 
\bar under
goal-oriented nature of METAM
\bar default
: when the base dataset is already strong, additional features are unnecessary,
 but when the base is weak, even modest improvements from candidate augmentation
s are recognized and selected.
 This behavior aligns with the paper‚Äôs discussion on optimizing query efficiency
 and achieving a minimal yet effective augmentation set.
\end_layout

\end_deeper
\begin_layout Enumerate

\bar under
Goal-oriented nature of METAM
\bar default
: Housing Price in Seattle Regression 
\end_layout

\begin_deeper
\begin_layout Itemize

\bar under
Base set:
\bar default
 seattle_housing_prices
\end_layout

\begin_layout Itemize

\bar under
Augementation candidates:
\bar default
 seattle_incomes, seattle_pet_licenses, seattle_crime_rates
\end_layout

\begin_layout Itemize

\bar under
Task:
\bar default
 predicting the actual housing price (a continuous variable) based on zip
 codes
\end_layout

\begin_layout Itemize

\bar under
Result:
\bar default
 selected augmentation: Crime Rate Data
\end_layout

\begin_layout Itemize

\bar under
Explanation:
\bar default
 The Crime Rate Data showed a moderate overall quality score (‚âà0.515), which
 was the lowest of all candidates, yet was the only candidate to yield a
 measurable improvement, raising the utility from 0.6562 to 0.6595.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
This report demonstrates the versatility and effectiveness of METAM's goal-orien
ted data augmentation approach across diverse scenarios.
 Our experiments reveal several important insights:
\end_layout

\begin_layout Itemize
Selective Augmentation: In cases where the base dataset is already highly
 predictive, such as in experiment 2, METAM correctly identifies that additional
 augmentation would be redundant.
 The algorithm‚Äôs minimality check ensures that only augmentations yielding
 a measurable improvement are integrated into the final dataset.
 This not only prevents overfitting by avoiding unnecessary complexity but
 also reinforces the notion that a well-engineered base dataset can sometimes
 be sufficient for achieving high performance.
\end_layout

\begin_layout Itemize
Sensitivity to Modest Gains: In scenarios with a relatively weak base dataset,
 such as in experiment 3, even modest improvements are recognized and exploited.
 This demonstrates METAM‚Äôs sensitivity to incremental benefits, highlighting
 its ability to fine-tune the feature set to boost performance.
 The adaptive querying mechanism dynamically balances exploration (evaluating
 multiple candidate augmentations) with exploitation (selecting the augmentation
 that most improves utility), aligning with the paper‚Äôs claims about efficient
 query optimization.
\end_layout

\begin_layout Itemize
Application to Regression Tasks: The Housing Price Regression experiment
 illustrates METAM‚Äôs capacity to extend beyond binary classification.
 By integrating external datasets (despite some candidates having lower
 initial quality scores), METAM was able to extract non-redundant, valuable
 information‚Äîevidenced by a measurable increase in the utility score.
 This confirms that METAM can be effectively applied to more complex, continuous
 prediction tasks, further broadening its applicability in real-world scenarios.
\end_layout

\begin_layout Itemize
Theoretical Consistency: Across all experiments, the behavior of METAM aligns
 with the theoretical foundations presented in the original paper.
 The algorithm‚Äôs reliance on data profiles‚Äîincluding semantic similarity‚Äîensures
 that candidate augmentations are evaluated on multiple relevant dimensions.
 Moreover, the clustering and minimality checks guarantee that only the
 most informative augmentations are retained.
 This selective process not only reduces computational overhead but also
 mitigates the risk of incorporating noisy or redundant data.
\end_layout

\begin_layout Standard
In summary, our implementation and experiments underscore the strength of
 METAM's adaptive, minimal, and goal-oriented approach to data augmentation.
 The framework successfully balances the trade-off between adding extra
 features and maintaining a parsimonious model, ultimately leading to enhanced
 predictive performance.
 These findings suggest that METAM can serve as a valuable tool in a variety
 of data-driven decision-making applications, from classification to regression
 tasks, by automating and optimizing the process of data augmentation.
\end_layout

\end_body
\end_document
